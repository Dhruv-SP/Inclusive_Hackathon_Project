{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL FIRST!\n",
    "%pip install -q langchain==0.0.150 pypdf pandas matplotlib tiktoken textract transformers openai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"{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}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Advanced method - Split by chunk\n",
    "\n",
    "# Step 1: Convert PDF to text\n",
    "import textract\n",
    "doc = textract.process(\"/Users/darshjoshi/Desktop/Jupyter Notebooks/Mahabharata (Unabridged in English).pdf\")\n",
    "\n",
    "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
    "with open('reference.txt', 'w') as f:\n",
    "    f.write(doc.decode('utf-8'))\n",
    "\n",
    "with open('reference.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 3: Create function to count tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Step 4: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 24,\n",
    "    length_function = count_tokens,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)\n",
    "type(chunks[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Algorithm For Data Science, Pace University\\nSpring 2023\\n\\nComparing K-means distance measures\\nRobustness to noise\\nDhruv Patel\\n\\nSung-Hyuk Cha\\n\\nDepartment of Computer Science\\nPace University\\n\\nDepartment of Computer Science\\nPace University\\n\\nNew York, USA\\ndp99977n@pace.edu\\n\\nNew York, USA\\nscha@pace.edu\\n\\nAbstract— This research paper investigates the robustness of\\nKmeans distance measures to noisy data. A common clustering\\napproach called Kmeans divides a dataset into K groups based on\\nhow similar the data points are to one another. The existence of\\nnoise in the data, however, can have a major impact on Kmeans'\\nperformance. we evaluate the robustness of various distance\\nmeasures used in Kmeans, including Euclidean, Manhattan, and\\nCosine distances, to different levels of noise in the data. Our\\nexperimental findings demonstrate that the choice of distance\\nmeasure has a significant impact on the performance of Kmeans,\\nand evaluating the best distance measure.\", metadata={}),\n",
       " Document(page_content='I. INTRODUCTION\\nClustering and classification are fundamental tasks in\\nsupervised as well as unsupervised machine learning models.\\nThe application spectrum for Kmeans is vast including use\\ncases as image segregation, customer segregation, Anomaly\\ndetection, Recommendation system, DNA micro array\\nanalysis and so on. Clustering has been the ML engineer’s\\nchoice when it comes to finding entities that fall similar to a\\ngroup of other entities. Knn and Kmeans had been a popular\\nchoice as these algorithms cluster data based on their nearest\\nK neighbours, and due to the nature of the algorithm, they can\\nalso be used to classify the data, unlike DBSCAN which we\\nwill discuss in short while.\\nThe common approach for all Machine learning modelling\\nprojects starts with data pre-processing, which include a\\ncrucial task of data cleaning. Data cleaning tasks include, null\\nvalue handling, type checking, outliers and so on. One part\\nwhere enough emphasis is required is outlier detection, for\\nmany datasets because of their nature, it gets confusing to\\ndecide whether the data is actually a noise or an extremely\\nmotivated characteristic of an entity. This requires lot more\\nresearch on data which is costly for a project manager. There\\nare approaches such as using DBSCAN that can deal with\\nnoisy data but due to the functionality of DBSCAN,\\nclassifying the data to their respective class quite impossible\\nin many cases.', metadata={}),\n",
       " Document(page_content=\"under shadowed by other true points in the K collection of any\\ndatapoint.\\nII. LITERATURE REVIEW\\nA. K – means and Knn Algorithm\\nTwo popular machine learning methods for data analysis\\nare K-means clustering and K-nearest neighbours (Knn).\\nWhile they both use distance measurements to group data\\npoints, they serve different purposes and employ different\\nstrategies. The comparison of the K-means and KNN\\nalgorithms is the main subject of this literature study, with a\\nfocus on their robustness to noisy data.\\nThe KNN classification method labels each data point\\nbased on the labels of its K closest neighbours in the training\\ndata. The method chooses the K closest points by calculating\\nthe distances between the data point and each training data\\npoint. The majority label of the data point's K nearest\\nneighbours is then used to determine the label for the data\\npoint. KNN is robust to training data noise, but it is susceptible\\nto test data noise, which can result in misclassification. Several\\nstrategies, such as the use of weighted distances or the\\nadjustment of local density estimates, have been suggested to\\nincrease the robustness of KNN to noisy data.\\nK-means, on the other hand, divides a set of data points\\ninto K clusters according to how similar they are. The\\nalgorithm updates the cluster canter based on the mean of the\\ndata points in each cluster after iteratively assigning each data\\npoint to the closest cluster canter. While K-means is good at\\nfinding clusters in data, it is also sensitive to data noise, which\\ncan cause spurious clusters to form or separate clusters to\\nmerge. The Canberra distance and other alternative distance\\nmeasures, as well as the use of robust statistics in place of the\\nmean, have all been suggested as ways to increase the\\nresilience of K-means to noisy data.\\n\\nIn this paper, we researched on weather it a good approach\\nfor a organisation to not invest time in researching the outlier\\nor they should accept the outlier and get on modelling as Knn\\nand Kmeans use multiple K neighbours so the outlier will be\\nImage 1. Kmeans\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\", metadata={}),\n",
       " Document(page_content=\"Algorithm For Data Science, Pace University\\nSpring 2023\\n\\nIn general, K-means and KNN use distance measures\\nsimilarly, although their goals and approaches are different.\\nKNN is utilized for classification, but K-means is frequently\\nused for clustering but is also used for classification. Both\\nalgorithms are sensitive to data noise, but methods have been\\nput forth to make them more robust. In general, the particulars\\nof the work and the features of the data will determine the\\nmethod and strategy to be used.\\n\\nB. DBSCAN\\nDensity-Based Spatial Clustering of Applications with\\nNoise is a popular clustering algorithm in machine learning\\nthat is known for its ability to handle datasets with arbitrary\\nshapes and sizes. In this examination of the literature, we will\\ndiscuss DBSCAN's salient characteristics, as well as its\\nadvantages and disadvantages in relation to the K-means\\nmethod.\\nDBSCAN clusters together points that have similar\\ndensities. A core point is one that the algorithm considers\\nhaving a minimal number of neighbours within an established\\nrange (epsilon). The same cluster is thought to contain all\\npoints that are within the epsilon distance of a core point. The\\ntechnique is also capable of locating noise points that do not\\nform a cluster.\\nDBSCAN does, however, have some restrictions. Its\\ninability to categorize data into predetermined groups or\\nclasses is one of its key drawbacks. DBSCAN is typically used\\nfor clustering, while K-means can be used for classification.\\nThe provision of two important parameters, epsilon and the\\nminimum number of points necessary to create a cluster, is also\\nneeded by DBSCAN. The selection of these parameters may\\ninvolve some trial and error and may have a substantial impact\\non how well the algorithm performs. Due to its more intricate\\ncalculations, DBSCAN generally performs slower than Kmeans.\", metadata={}),\n",
       " Document(page_content=\"C. Elbow method\\nThe elbow approach is a well-liked way for figuring out\\nhow many clusters in a dataset are ideal for algorithms like Kmeans clustering. We will examine the elbow method's main\\ncharacteristics, its advantages and disadvantages, and how it\\nstacks up against other methods for figuring out the ideal\\nnumber of clusters in this literature survey.\\nThe elbow method works by plotting the within-cluster\\nsum of squares (WCSS) against the number of clusters. The\\ntotal of the squared distances between each data point and the\\ndesignated cluster center is what the WCSS calculates. The\\npoint of inflection on the elbow-shaped plot of the WCSS\\nagainst the number of clusters represents the ideal number of\\nclusters. This is due to the fact that decreasing the number of\\nclusters below this point causes a marked increase in the\\nWCSS while increasing the number of clusters above it causes\\na diminishing reduction in the WCSS.\\nThe simplicity and usability of the elbow approach are two\\nof its main benefits. Implementing the procedure is simple and\\ndoesn't call for a lot of computing power. The elbow method\\nis also popular and has been utilized with numerous datasets\\nand clustering techniques.\\nThe silhouette method and the gap statistic are two other\\napproaches that have been suggested to find the ideal number\\nof clusters. The gap statistic compares the WCSS of the\\nobserved data to a null reference distribution produced by a\\nrandom process, whereas the silhouette approach assesses how\\nsimilar a data point is to its own cluster in comparison to other\\nclusters.\\n\\nImage 3. WCSS\\n\\nThe elbow approach is a popular and straightforward\\nmethod for figuring out the ideal number of clusters in a\\ndataset. Despite few drawbacks, it is still a common alternative\\nbecause of how simple it is to use and how widely it is used.\\nFor some datasets and clustering algorithms, alternative\\ntechniques, including the silhouette approach and gap statistic,\\ncan also be used to estimate the ideal number of clusters.\\n\\nImage 2. DBSCAN vs Kmeans\", metadata={}),\n",
       " Document(page_content=\"Image 2. DBSCAN vs Kmeans\\n\\nIn conclusion, even though DBSCAN offers a number of\\nadvantages, such as the capacity to handle datasets of any\\nshape or size and its resistance to noisy data, it cannot always\\nbe used in place of K-means. For categorization, K-means can\\nbe employed and is typically quicker than DBSCAN. The\\nparticular task at hand and the properties of the data will\\ndetermine which algorithm is used.\\n\\nD. Euclidean Distance\\nA common distance metric used in machine learning to\\nassess how similar two data points are is euclidean distance.\\nThe straight-line distance in Euclidean space between two\\nplaces is known as the Euclidean distance. The total of the\\nsquared differences between each coordinate of the two points\\nis used to calculate it. In clustering algorithms like K-means,\\nhierarchical clustering, and DBSCAN, Euclidean distance is\\nfrequently used.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\\n\\nspace, the Manhattan distance is particularly helpful when\\nworking with high-dimensional data.\\n\\nImage 4. Euclidian Distance\\n\\nEuclidean distance's simplicity and use are two of its main\\nbenefits. The computation is simple, widely accepted by the\\nmachine learning community, and easily understood.\\nEuclidean distance is also simple to understand and intuitive.\\nHowever, there are some restrictions on Euclidean distance. Its\\npresumption that all aspects or features of the data are equally\\nsignificant is one of its limitations. Euclidean distance may not\\nalways produce the best results when some dimensions are\\nmore crucial than others. Furthermore, Euclidean distance may\\nnot function well in datasets with high-dimensional spaces and\\nmay be sensitive to outliers.\\nE. Squared Euclidian Distance\\nA variation of Euclidean distance, frequently employed in\\nmachine learning techniques like K-means clustering. The\\ntotal of the squared differences between each coordinate of two\\ndata points is used to determine the squared Euclidean distance\\nbetween two points. The squared Euclidean distance is faster\\nto calculate than the Euclidean distance because it is not\\nrooted. When speed is a factor, Squared Euclidean distance is\\nfrequently used as a rough approximation of Euclidean\\ndistance.\", metadata={}),\n",
       " Document(page_content=\"Manhattan distance, also referred to as taxicab distance, is\\na distance measure that is frequently employed in machine\\nlearning techniques like clustering and K-nearest neighbours.\\nThe total of the absolute differences between each coordinate\\nof two data points is used to determine the Manhattan distance.\\nIt is less susceptible to outliers than Euclidean distance\\nbecause it does not calculate with squares or square roots.\\nBecause it does not distort distances in high-dimensional\\nspace, the Manhattan distance is particularly helpful when\\nworking with high-dimensional data.\\n\\nImage 6. Manhattan Distance\\n\\nIn conclusion, Manhattan distance is a straightforward and\\nuncomplicated distance metric that is appropriate for highdimensional data and less sensitive to outliers. It does,\\nhowever, have limitations in that it does not take feature\\ncorrelations into account and treats each characteristic equally.\\nFor some datasets and machine learning tasks, other distance\\nmeasures might be better suitable.\\n\\nThe squared Euclidean distance's computational efficiency\\nis one of its key benefits. The computation of the distance\\nmetric is quicker than Euclidean distance since it does not\\ninvolve the square root. Additionally, because squared\\ndifferences magnify the impact of significant deviations,\\nSquared Euclidean distance is less susceptible to outliers than\\nEuclidean distance.\\n\\nG. Chebyshev Distance\\nThe Chebyshev distance metric, usually referred to as the\\nmaximum distance or L∞ distance, is frequently employed in\\nmachine learning methods like clustering and K-nearest\\nneighbours. The biggest absolute difference between the\\ncoordinates of two data points is used to determine the\\nChebyshev distance. It is less susceptible to outliers than\\nManhattan and Euclidean distances since it only takes into\\naccount the biggest difference between the coordinates,\\nmaking it ideal for data with extreme values. It is especially\\nhelpful in applications like control theory where the maximum\\ndeviation is important.\\n\\nImage 5. Squared Euclidian Distance\\n\\nImage 7. Chebyshev Distance Max\\n\\nSquared Euclidean distance is quicker to compute than\\nEuclidean distance, but it does not maintain the actual\\ndistances between data points as well. In general, Euclidean\\ndistance is more precise and better suited for low-dimensional\\ndatasets when computational performance is unimportant.\", metadata={}),\n",
       " Document(page_content=\"Chebyshev distance's resistance to outliers and capacity for\\nhandling data with extreme values are two of its primary\\nfeatures. It is helpful in applications like control theory where\\nthe biggest deviation is important. It is also a straightforward\\nand effective distance measure in terms of calculation.\\nChebyshev distance does, however, have some restrictions. Its\\ninability to account for the magnitudes of the coordinate\\ndisparities is one of its drawbacks. Additionally, it equalizes\\nall of the data's features or dimensions, which may not be\\nappropriate for all datasets.\\n\\nF. Manhattan Distance\\nManhattan distance, also referred to as taxicab distance, is\\na distance measure that is frequently employed in machine\\nlearning techniques like clustering and K-nearest neighbours.\\nThe total of the absolute differences between each coordinate\\nof two data points is used to determine the Manhattan distance.\\nIt is less susceptible to outliers than Euclidean distance\\nbecause it does not calculate with squares or square roots.\\nBecause it does not distort distances in high-dimensional\\n\\nChebyshev distance is more appropriate for data with\\nextreme values and is less sensitive to outliers than Euclidean\\ndistance. It may not be appropriate for all datasets and does not\\nretain the actual distances between data points as well.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\\n\\nsatisfies the triangle inequality and is a symmetric distance\\nmetric.\\n\\nImage 8. Chebyshev Distance infinite\", metadata={}),\n",
       " Document(page_content=\"Image 8. Chebyshev Distance infinite\\n\\nIn conclusion, for data with extreme values and\\napplications where the highest deviation is crucial, Chebyshev\\ndistance is a reliable distance metric. Alternative distance\\nmetrics might be more appropriate for some machine learning\\ntasks, and it might not be appropriate for all datasets.\\nH. Canberra Distance\\nCanberra distance is a typical distance metric in machine\\nlearning techniques like clustering and K-nearest neighbours.\\nThe distance between two points in Canberra is computed by\\ndividing the total absolute difference in the coordinates of the\\ntwo points by the total of their absolute values. It is better\\nsuitable for datasets with widely changing magnitudes\\nbecause, unlike Euclidean and Manhattan distances, it\\nconsiders the magnitudes of the discrepancies between\\ncoordinates. Applications for feature selection and data\\ncompression benefit the most from it.\\nThe capacity to handle datasets of widely different\\nmagnitudes is one of Canberra distance's key features, which\\nmakes it particularly helpful in feature selection and data\\ncompression applications. It is a computationally effective\\ndistance metric as well. Canberra distance does, however, have\\nsome drawbacks. Its vulnerability to extremely small values,\\nwhich might result in division by zero or numerical instability,\\nis one of its limitations. Additionally, it ignores feature\\ncorrelations, which in some circumstances could lead to lessthan-optimal clustering solutions.\\n\\nHowever, Chi-square distance also has some drawbacks.\\nOne is that it may not be suitable for datasets with negative\\nvalues, as it requires non-negative data. Additionally, it may\\nbe sensitive to differences in the number of dimensions or bins\\nin the histograms being compared.\\n\\nImage 10. Chi-Square Distance\", metadata={}),\n",
       " Document(page_content='Image 10. Chi-Square Distance\\n\\nIn comparison to Euclidean distance, Chi-square distance\\nis more appropriate for comparing histograms or probability\\ndistributions. However, it may not be suitable for all datasets,\\nand alternative distance metrics may be more appropriate for\\ncertain machine learning tasks.\\nJ. Purity score\\nPurity score is a measure of cluster quality used in k-means\\nclustering. The degree to which a cluster exclusively includes\\none class label is indicated by the purity score. It is determined\\nas the total number of data points divided by the sum of the\\ngreatest class frequency for each cluster. Purity score is easy\\nto grasp and interpret and is extensively utilized in practice.\\nThe ease of use and interpretability of purity score is one\\nof its key benefits. It offers an easy-to-understand gauge of\\ncluster quality that can be shared with stakeholders. It is also a\\nquick and effective measure in terms of calculation.\\nHowever, there are certain restrictions on the purity score,\\nthough. The fact that it presumes that the ground truth labels\\nare known which is not always the case in practice is one of its\\nlimitations. Furthermore, because of potential bias toward the\\ndominant class, it might not be suitable for datasets with class\\nimbalance.\\n\\nImage 9. Canberra Distance\\n\\nIn conclusion, the Canberra distance is a metric for\\nmeasuring distance that takes the magnitudes of coordinate\\ndiscrepancies into consideration. It is particularly helpful in\\napplications for feature selection and data compression.\\nHowever, it is susceptible to very small values and ignores\\nfeature correlations. For some machine learning tasks,\\ndifferent distance measurements might be better suitable.\\nI. Chi-Square Distance\\nChi-square distance is a popular distance metric in machine\\nlearning techniques like clustering and K-nearest neighbours.\\nThe sum of the squared differences between the coordinates of\\ntwo data points divided by their sum is used to determine the\\nchi-square distance. It is especially helpful in applications that\\ncompare probability distributions or histograms.\\nChi-square distance is advantageous in applications like\\nimage processing and computer vision due to its ability to\\ncompare histograms or probability distributions. It also', metadata={}),\n",
       " Document(page_content='In contrast to other cluster quality metrics, purity score is a\\nstraightforward and understandable indicator of cluster\\nquality. Alternative measures, such as the adjusted Rand index\\nor F1 score, may be more suitable for some machine learning\\ntasks and may not be appropriate for all datasets.\\n\\nImage 11. Purity Score\\nN = number of objects (data points), k = number of clusters, ci is a\\ncluster in C, and tj is the classification which has the max count for\\ncluster ci.\\n\\nIn conclusion, the purity score is a straightforward and\\ninterpretable measure of cluster quality utilized in k-means\\nclustering. For machine learning projects with special needs,\\nother approaches should be taken into account because it has\\nsome limits.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\\n\\nIII. IMPLEMENTATION\\nA. Dataset\\nIris dataset is a popular choice to benchmark clustering and\\nclassification machine learning models. The reason for the\\nsame is that the data is simple, the data is denser in their range\\nand most of all, it’s perfectly balanced. There are in total 150\\ninstances contain facts about 3 different flowers {Setosa,\\nVersicolor, and Virginica} including the sepal length, sepal\\nwidth, petal length, and petal width with all the three flowers\\nhaving 50 entries of each.\\nImage 13a.\\n\\nImage 12. Iris\\n\\nThe dataset was first used in a 1936 study titled \"The use\\nof multiple measurements in taxonomic problems\" by British\\nstatistician and biologist Ronald Fisher. Fisher demonstrated a\\nlinear discriminant analysis method to categorize the three iris\\nspecies based on their morphological traits using the Iris\\ndataset.\\n\\nImage 13b.', metadata={}),\n",
       " Document(page_content='Image 13b.\\n\\nWith one row for each iris flower and four columns for the\\nfour physical measures, the iris dataset is organized as a table.\\nThe species name for the iris flower in each row is also\\nincluded. The dataset is frequently used as a dummy example\\nfor machine learning clustering, classification, and\\nvisualization tasks.\\nIII.A.1 Data challenge\\nIris dataset is overall a simple dataset but there is one thing\\nto notice before getting started with model. The main\\nchallenge is cluster conflict, cluster conflict is when two\\nclusters collide with each other. In our dataset, versicolor and\\nvirginica are colliding for all the features where as Setosa is\\nspread apart from both the other class for all features.\\nThis can result in conflict for class determination as Kmeans work by taking in consideration the majority class\\ndatapoints in the K closest set and assign the datapoint in\\nquestion to the majority class. This nature can result in\\nmisclassification of some data points resulting in affecting the\\nscore of the model. This is also a good thing for this research\\nas the data challenge will thoroughly test the model for all the\\nnoise we set and the observed score will be equally affected.\\n\\nImage 13c.\\n\\nIn the above scatter plot, it can observe that the Setosa sits\\nseparate from versicolor and virginica by some margin across\\nall the cross-feature scatter plots. Also, thing to notice is that\\nversicolor and virginica collide among many features.\\nHowever, the denser distribution is still separate, there is a thin\\nline where the minimum of one flower feature is similar to\\nmaximum flower feature of another. The task here will be to\\ncalculate the distance and provide adequate importance to that\\ndistance so that the correct class datapoints are in majority.\\nThis will be a good test for all the distance measures.', metadata={}),\n",
       " Document(page_content='B. Base lines and utilities\\nBefore starting with the modelling, we need to establish\\nsome base line parameters so that the evaluation can be done\\nwith the noisy data. To set the baseline, we first used the elbow\\nmethod to determine the perfect value for K which will be used\\nthroughout the experiment. Then using that K value, we will\\nobserve the purity score for default Kmeans algorithm\\nprovided by the sklearn library in python. Then plotting the\\nclusters with centroids to see the class separation, mainly how\\nwell virginica and versicolor are separated.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\\n\\nIII.B.1 Elbow method\\nElbow method, as discussed, is used to find the optimum\\nvalue of K for the Kmeans algorithm. we used the\\nKElbowVisualizer() procedure provided in sklearn library as\\nits much simpler to implement and the results are adaptable. In\\norder to get the procedure to execute, one needs to first define\\nthe model and train the model on the train data using .fit()\\nprocedure. After executing the, the results are presented in the\\nbelow figure.\\n\\nImage 15. clustering\\n\\nImage 14. Elbow method', metadata={}),\n",
       " Document(page_content='Image 15. clustering\\n\\nImage 14. Elbow method\\n\\nThe figure plotted by the procedure depicts that the\\noptimum value of K is 3 and the received score is 78% for\\ntraining data. The reason for the value to be 3 is that the\\ndistance slop at 3 starts to get constant, before 3 the slop is\\ndropping at some high rate.\\nIII.B.2 Baseline model score\\nThe criteria for the baseline model is that the data has to be\\nclean and original, no standardisation or any data preprocessing, no noise added, using the default distance measure\\nprovided by the sklearn and using the k value identified\\noptimum by the elbow method. The idea here is so that we can\\nget some baseline of how the default model works with the\\ndefault distance measure so that we have come parameter to\\ncompare the experiment results with something other than\\nitself. We executed the Kmeans using fit_predict() as it wont\\njust train the data but also predict the clusters centroids for\\neach class. After execution we calculated the purity score of\\nthe prediction which came out to 89.33 %. This score will be\\nthe baseline for our experiment.\\nIII.B.3 Cluster centres\\nSklearn has the procedure as model.cluster_centers_\\nwhich returns an array containing the value for all the clusters\\nthat will be used to plot the graph. After getting the clusters,\\nwe used the matplotlib library to plot the clusters using\\nthe .scatter() procedure. The resulting graph obtained is\\nbelow.', metadata={}),\n",
       " Document(page_content='In the graph, all the cluster centers are plotted as “+”, the\\ndifferent coloured points belong to the respective class. As\\nexpected, setose did not possessed any challenge to the\\nmodel, the cluster center feels nice in the mean middle of the\\ndata. Versicolor and virginica, as expected, did conflict with\\neach other but the model was able to predict the classes\\nnicely, it can also be seen that few of the virginica are\\npredicted as versicolor as there are more versicolor in the\\nplot, but the data had them in equal ratio.\\nC. Adding noise\\nIn the experiment, we’ll iteratively add noise to the data at\\nthe rate of 1% and will go till 25%, which means that we will\\nbe adding noise quiet a lot of time, to keep the whole program\\nmodular, we created a module or procedure that takes in the\\nnumber of error we want to add and will return the train data\\nand test data with defined numbers of outliers in the data. This\\nmakes the program clean and highly modular.\\nIII.C.1 Noise criteria\\nBefore creating the algorithm, we discussed what sort of\\nnoise to be added, and as described in the Introduction, the best\\nsort of noise that can be added is outliers. We also don’t want\\nthe noise to be very extreme, so the criteria decided for the\\nnoise was that the noise value has to be <min feature value\\nand >max feature value. This ensures that the noise will not\\nget mixed with true data points and not too extremely separate\\nthat it becomes easily noticeable and ruins the model just from\\nthe start.\\nIII.C.2 Input and output of the get_noisy_dataframe\\nmodule.\\nThe input to the module will be the rate of error or number\\nof errors that will be added to each feature, which will be\\ncalculated by get_percent_values() which takes in the %age\\nvalue and return the absolute number of error.\\nThe output of the module will be the X data or the train\\ndata containing the 4 feature values for all the flowers and Y\\ndata or the test data which contains the label values of all the\\nflowers.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023', metadata={}),\n",
       " Document(page_content='Algorithm For Data Science, Pace University\\nSpring 2023\\n\\nIII.C.3 Algorithm\\nThe detailed pseudo code is defined below:\\nget_noisy_dataframe(error_rate)\\n1. get data as iris\\n2 for each feature, perform:\\n3. get feature values in feature_column\\n4. get max value in feature_column as MAX\\n5. get min value in feature_column as MIN\\n6. declare counter as xx\\n7. while xx < error_rate, perform, else: go to 14.\\n8. generate random value between 0,10\\n9. if random_value<MIN or random_value>MAX, then continue,\\nelse: go to 8.\\n10. get random row index as ran_row_ind\\n11. assign random_value at feature_column[ran_row_ind]\\n12. increment counter xx\\n13. return to 7.\\n14. replace column in iris dataset with feature_column\\n15. split iris dataset in to X and Y\\n16. return (X,y)\\n\\nImage 18. 6 error\\n\\nAt 6 error per column, totalling 24 error in total, the purity\\nscore dropped from 86% to 65%, significant deduction. The\\nreason here is that the noise points added moved the cluster\\ncenter towards other cluster, resulting in misclassification of\\ndata points. And as expected, virginica and versicolor did\\nconflicted with each other and most of the datapoints in one\\nclass are classified as others.\\n\\nD. Evaluating noise to centroid\\nAfter generating the noisy data, we need to verify as if the\\nnoise is affecting the model at all. So, we again ran the baseline\\nmodel with iteratively adding the noise and observing the\\nresult. After each run, we plotted the clusters with centroids to\\nobserve the impact.\\nImage 19. 9 error\\n\\nAt 9 error per column, totalling 36 error in total, the purity\\nscore increased from 65% to 84%, significant increase. The\\nreason for this unexpected change is that the noise is generated\\non random and in this run, the noise is not too far from the\\ncluster, so the centroids where not shifted that far and resulting\\nin conflicting with other clusters.\\nImage 16. 0% error', metadata={}),\n",
       " Document(page_content='At 0 error per column, the results are as expected, similar\\nto what we observed during the baseline.\\n\\nImage 20. 12 error\\n\\nImage 17. 3 error\\n\\nAt 3 error per column, totalling at 12 error in total, the\\npurity score dropped from 89% to 86%, the deduction isn’t\\nhuge as the noise points added are far from the clusters, so the\\nmean centroids where not affected much. The cluster centroids\\nare still close to the center of the clusters.\\n\\nAt 12 error per column, totalling 48 error in total, the purity\\nscore decreased from 84% to 63%, significant decrease. The\\nreason here is similar to 6 error per column. The reason its not\\nin line with 9 error per column is also same, the error generated\\nis random, so this case was not as good as 9 error per column\\nfor the model.\\nIII.D.1 Error conclusion\\nAs we see the above results, the conclusion can be made\\nthat the model is very sensitive to the noise when Euclidian\\ndistance is used. In order to deal with the issue of random\\nworst-case scenario, we need to make multiple runs for the\\nsame error range and then take the average of all those runs,\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023\\n\\nthat way the rare worst case or best case scenario will be\\ndimmed down against all the average case scenarios.', metadata={}),\n",
       " Document(page_content='that way the rare worst case or best case scenario will be\\ndimmed down against all the average case scenarios.\\n\\nE. Algorithm\\nThe main module that is responsible to perform all the\\nexperiment, named run(), is on top of 3 other modules, first is\\nthe get_percent_value() that takes in the %age value and return\\nthe absolute error that need to be added, then we have\\nget_noisy_dataframe() that takes in the error rate and return a\\ndataframe with that many errors in it, and at last we have\\npyRunKm() that takes in the X data, the y data and the distance\\nmeasure and returns purity score that the Kmeans model\\ngenerated on the given data and the given distance measure.\\nThe run() module takes in two values, the min percentage error\\nand the maximum percentage error, and it iteratively keeps\\nadding noise at a constant rate till it reaches maximum\\npercentage error, it returns a 3D array, one dimension for the\\nnumber of run which will be 0 to 100, one dimension for each\\nseparate errors score, and the last dimension for the distance\\nmeasure.\\nIII.E.1 Pseudo code for run()\\nrun(min_error_percent,max_error_percent)\\n1. declare a list as error_list = []\\n2 for each run_error_percentage in range (min_error_percent,\\nmax_error_percent), perform: 3-9\\n3.get error_rate from get_percent_values(run_error_percent)\\n4. get X,y from get_noisy_dataframe(error_rate)\\n5. declare a temperary list as temp_list = []\\n6. for each distance_measure, perform 7-8\\n7. get purity from pyRunKm(X,y,value)\\n8. append purity to temp_list\\n9. append temp_list to error_list\\n10. declare list for each distance\\n11. append respective list from error_list to distance_list\\n12. return error_list', metadata={}),\n",
       " Document(page_content='IV. RESULTS\\nThe modules performed respectively slower than expected\\nas total of 25 (total percent value) * 6 (total distance measures)\\n* 100 (total runs performed) = 15,000 executions where done.\\nAfter doing 100 runs for error rate ranging from 0% to 25% in\\neach column, large set of purity scores where obtained. First,\\nwe put all the runs to a plot to check the variation in purity\\nscore based on random best-case and worst-case scenarios\\nappeared due to randomly generated noise.\\n\\nImage 21. 100 run overlap\\n\\nAs expected, a lot of worst-case and best-case scenarios\\nhad happened during the 2,500 (%values * total runs) runs.\\nThere are sudden down spikes that are returning back to up and\\nthere are lot of up spikes that are returning down. One thing to\\nnotice is that the hue of yellow is denser over all the other\\ncolour and is performing better then other distance measures,\\nthat distance measure is, Canberra distance measure.\\nThis graph is not sufficient to draw strong conclusions as\\nthere are many best- and worst-case scenarios taking place. So,\\nto get a meaningful graph, we took the average of all the 100\\nruns for all the distance measures overt the range of percentage\\nerror. The graph we obtained is followed.\\n\\nImage 22. 100 run average\\n\\nThis graph is of the average purity score obtained over 100\\nruns. Its clear that the Canberra distance measure is performing\\nbest over any error rate. Following we have the Manhattan\\ndistance measure performing better than other measures but\\nstill significantly poor than Canberra over the range of 5% to\\n25%. In the middle there is a mix between chi-square, squared\\nEuclidian, and Euclidian. And the worst performing measure,\\nChebyshev distance measure. One interesting thing to notice is\\nthat the Euclidian and the squared Euclidian perform just the\\nsame over all the range. The detailed insights are as followed.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023', metadata={}),\n",
       " Document(page_content='Algorithm For Data Science, Pace University\\nSpring 2023\\n\\n• Starting with the best performing, Canberra distance\\nmeasure, which kept the score of 61%+ even at 25%\\nerror per column. The significant drop was observed at\\n5% to 6% error where the score dropped from 82% to\\n69%.\\n• The second-best performing measure was Manhattan\\ndistance measure, holding the score at 56%. The\\nsimplicity of this measure can be a base to use it over\\nany other distance.\\n• Then we have Euclidian and squared Euclidian\\nperforming exactly the same over all the range of error.\\nand as they are performing the same, squared Euclidian\\ndistance should be used as its less computationally\\ncostly.\\n• Then we have chi square distance measurements, it\\nperformed best for 0% error data for all the 100 runs\\nbut as there was 1% error, the score went from 97% to\\n86%, major score loss. Chi squared suffered the biggest\\nloss going from 5% error to 6% error, which makes chi\\nsquare the least robust distance measure and the best\\nperforming distance measure at 0%.\\n• At last we have Chebyshev distance measure, the least\\nrobust distance measure and the worst performing one.', metadata={}),\n",
       " Document(page_content='V. CONCLUSION\\nWe used the Iris dataset that contained 150 instances of 4\\nfeatures and 3 classes having 50 instances for each class. The\\nnoise criteria we selected was to generate a random value that’s\\nbetween 0 to 10 and is not in the range of feature values. We\\ndid runs for 0% error per column to 25% error per column for\\nall the 6 distance measures and averaged the purity score of\\n100 different runs. The results obtained after the experiment\\nstate that the Kmeans machine learning algorithm is not robust\\nin handling the noisy data, just by adding 1% noise to each\\ncolumn dropped the score significantly. Over the period of\\niteratively adding the error, the score kept decreasing spikily\\nuntil it was 5%, then the drop in score of marginal over each\\niteration. All the distance measures performed differently.\\nBy observing all the insights, it can be concluded that the\\nmost robust distance measure for Kmeans for noisy data is\\nCanberra distance measure and the least robust distance\\nmeasure is Chebyshev distance measure. All the distance\\nmeasurements suffered great score loss from 5% error to 6%\\nerror. All the distance measure gained some score going from\\n1% error to 2% error.\\nGoing back to the question, is it a good idea for project\\nmanagers to skip researching on outliers and reduce the project', metadata={}),\n",
       " Document(page_content='cost? Until the error is in minor range, i.e., 1% to 3%, it won’t\\nmake a significant impact on model’s performance, investing\\nthat time to experimenting with standardised data, min-max\\nnormalization, feature generation or any other data preprocessing task can give greater returns for the project.\\nVI. FUTURE WORK\\nKmeans and Knn are widely used machine learning\\nalgorithms when it comes to clustering and classification.\\nKmeans perform very well under clean data but as the noise\\nstarts to get in the training data, nonlinear loss of score is\\nobserved, however, this experiment was done on un-processed\\ndata. And the distance measure used where unweighted. There\\nare many experiments that can be conducted on similar cases.\\nSome of the possible future experiments can include:\\n1. Varying K value: the elbow method provides the\\noptimum K value for model; however it is possible that\\nchanging the K value respective to the noise can provide\\ndifferent results as more datapoints are taken into\\nconsideration and that can change the classification\\nprobability. So, to perform elbow method after every\\nerror introduction iteration.\\n2. Weighted distance measures: all the distance measures\\nused where unweighted, so the importance to a\\nneighbouring point just next to the datapoint in question\\nhas equal emphasis compared to the one that the\\nfarthest. Weighted distance can improve the chances of\\ncorrect classification as it makes sense that two alike\\npoints will be closes to each other.\\n3. Considering different dataset: A different dataset with\\ndifferent characteristics can impose different result. By\\napplying the same experiment on different dataset can\\nbe used to confirm the conclusions drawn from this\\nexperiment.\\n4. Run selection: we have done 100 runs on the same\\ndataset and for many runs, we were able to see some\\nextreme cases of sudden score gain or sudden score loss.\\nThis was because of the random value generated as\\nnoise, we can modify the program by checking the\\nvariance in score over all the error rate and decline the\\none that are having very high variance, this can improve\\nthe ratio of meaningful run we get at the end.\\n\\n\\x0cAlgorithm For Data Science, Pace University\\nSpring 2023', metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick data visualization to ensure chunking was successful\n",
    "\n",
    "# Create a list of token counts\n",
    "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Create a DataFrame from the token counts\n",
    "df = pd.DataFrame({'Token Count': token_counts})\n",
    "\n",
    "# Create a histogram of the token count distribution\n",
    "df.hist(bins=40, )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"text-embedding-ada-002\"\n",
    "\n",
    "# Get embedding model\n",
    "embeddings = OpenAIEmbeddings(model=model, openai_api_key=\"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\")\n",
    "\n",
    "# Create vector database\n",
    "db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1, openai_api_key=\"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\"), db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.llms.openai.OpenAI"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Capgemini Finance chatbot! Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/zgdqqntj4592_ppdbkfkfkyc0000gn/T/ipykernel_68548/2446587980.py:20: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f179533b3d64368ae578a07370b0092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f9c411f4d045a8824312b9e2e73f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> How many husbands does draupadi has? ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5abfb7c47644a319caa2e6cac621b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  Draupadi has five husbands.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492552ac5f4249ff997a5b32943ecaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> Talk to me like Karna : what are values for life? ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696900a25a3a4738aff828211244c1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  Karna values truth and virtue more than life itself and…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f80c1b7933f4a8eb07db52d701965d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> who is the blind guy in mahabharata?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8509da01af54eee8bcbdc077c5eb83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The blind character in the Mahabharata is Dhritarashtra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e9c0368b441ff8f60ca30391d0afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> Answering as Dhritarashtra, how do you feel about Karna?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf1b2d8c5c540268884ccb3f0f59890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  Dhritarashtra has a great affection for Karna and is pl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    input_box.value = \"\"\n",
    "    \n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thank you for using CapGenmini Finance chatbot!\")\n",
    "        return\n",
    "    \n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append((query, result['answer']))\n",
    "    \n",
    "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
    "\n",
    "print(\"Welcome to the Capgemini Finance chatbot! Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
